import { generateWithGrok } from '../../integrations/grok.mjs'
import { composeSystemPrompt } from '../../agents/prompt.mjs'
import { salvarSaidaEEnviar } from '../../dominio/mensagens/persistencia.mjs'
import { generateAndStoreSummary } from '../../dominio/conversas/resumo.mjs'
import { buildLLMMessages } from '../../dominio/llm/historico.mjs'
import crypto from 'node:crypto'
import { gerarImagemNSFW } from '../../integracoes/ia/image-generator.mjs'
import { checkImageAllowance, consumeImageQuota, hasActiveSubscription } from '../../assinaturas/controle.mjs'
import { getPersonaPhysicalTraits } from '../../dominio/personas/prompt-foto.mjs'
import { uploadImagemPublicaSupabase, listarPublicUrlsSupabase } from '../../integracoes/supabase/cliente.mjs'
import { resolveImagePrompt } from './resolve-image-prompt.mjs'
import { gerarAvatarFromConsistencyPack, gerarConsistencyPack } from '../../dominio/personas/consistency-pack.mjs'
import { audioModal } from '../../integracoes/ia/audio-modal.mjs'
import { audioQwen3Modal } from '../../integracoes/ia/audio-qwen3-modal.mjs'
import { uploadAudio } from '../../integracoes/supabase/storage-audio.mjs'
import { voiceManager } from '../../servicos/voice-manager.mjs'
import { join } from 'node:path'
import { descreverImagemGrok } from '../../integracoes/ia/grok-vision.mjs'

const WHATSAPP_FALLBACK_BLOQUEIO_CONTEUDO = (process.env.WHATSAPP_FALLBACK_BLOQUEIO_CONTEUDO || '')
  .toString()
  .trim()
  .slice(0, 1024) || 'Desculpa, eu n√£o consigo responder esse tipo de mensagem. Se quiser, me manda outra e eu te ajudo.'

const WHATSAPP_FALLBACK_ERRO_GERACAO = (process.env.WHATSAPP_FALLBACK_ERRO_GERACAO || '')
  .toString()
  .trim()
  .slice(0, 1024) || 'Mensagem recebida. Em breve sua Crush responde.'

const AUDIO_COST_MULTIPLIER = Math.max(1, parseInt((process.env.AUDIO_COST_MULTIPLIER || '10').toString(), 10) || 10)
const AUDIO_MAX_CHUNKS = Math.max(1, parseInt((process.env.AUDIO_MAX_CHUNKS || '6').toString(), 10) || 6)
const AUDIO_MAX_CHARS_PER_CHUNK = Math.max(120, parseInt((process.env.AUDIO_MAX_CHARS_PER_CHUNK || '180').toString(), 10) || 180)
const TTS_ENGINE_DEFAULT = (process.env.TTS_ENGINE_DEFAULT || 'xtts').toString().trim().toLowerCase()
const TTS_ENGINE_FALLBACK = (process.env.TTS_ENGINE_FALLBACK || 'xtts').toString().trim().toLowerCase()

function resolveTtsEngines() {
  const allowed = new Set(['xtts', 'qwen3'])
  const list = []
  const push = (v) => {
    const key = (v || '').toString().trim().toLowerCase()
    if (allowed.has(key) && !list.includes(key)) list.push(key)
  }
  push(TTS_ENGINE_DEFAULT)
  push(TTS_ENGINE_FALLBACK)
  if (!list.length) list.push('qwen3', 'xtts')
  return list
}

function clampText(s, maxLen) {
  const raw = (s || '').toString().trim()
  if (!raw) return ''
  const limit = Number.isFinite(Number(maxLen)) ? Number(maxLen) : 0
  if (!limit || limit <= 0) return raw
  return raw.length > limit ? raw.slice(0, Math.max(0, limit - 1)).trimEnd() : raw
}

function buildCaptionFallback({ personaName, poseType, closeUp }) {
  const t = (poseType || '').toString().trim().toLowerCase()
  const name = (personaName || '').toString().trim()
  const prefix = name ? `${name}: ` : ''
  if (closeUp) {
    if (t.startsWith('pussy')) return `${prefix}bem de pertinho‚Ä¶`
    if (t.startsWith('anal')) return `${prefix}bem de pertinho‚Ä¶`
    if (t.startsWith('breasts')) return `${prefix}bem de pertinho‚Ä¶`
    if (t.startsWith('butt')) return `${prefix}bem de pertinho‚Ä¶`
    return `${prefix}bem de pertinho‚Ä¶`
  }
  if (t === 'doggystyle') return `${prefix}de quatro pra voc√™.`
  if (t === 'metalstocks') return `${prefix}algemada pra voc√™.`
  if (t === 'shibari') return `${prefix}amarrada pra voc√™.`
  if (t === 'standing') return `${prefix}do jeitinho que voc√™ pediu.`
  if (t === 'lying') return `${prefix}do jeitinho que voc√™ pediu.`
  return `${prefix}do jeitinho que voc√™ pediu.`
}

function isCloseUpFromPoseType(poseType) {
  const t = (poseType || '').toString().trim().toLowerCase()
  return t.startsWith('pussy') || t.startsWith('anal') || t.startsWith('breasts') || t.startsWith('butt')
}

async function buildCaptionFromImage({ buffer, mimeType, personaName, poseType, closeUp, hint }) {
  const enabledRaw = (process.env.IMAGE_CAPTION_VISION || 'true').toString().trim().toLowerCase()
  const enabled = enabledRaw !== 'false' && enabledRaw !== '0' && enabledRaw !== 'no'
  if (!enabled || !buffer) return { ok: false }

  const poseHint = (poseType || '').toString().trim().toLowerCase()
  const poseLine =
    poseHint === 'doggystyle'
      ? 'Pose sugerida: de quatro (vista por tr√°s), sem rosto.'
      : poseHint === 'breasts'
        ? 'Pose sugerida: close no tronco, sem rosto.'
        : poseHint === 'butt'
          ? 'Pose sugerida: close por tr√°s, sem rosto.'
          : poseHint
            ? `Pose sugerida: ${poseHint}.`
            : ''

  const prompt = [
    `Crie UMA legenda curta (at√© 120 caracteres) em primeira pessoa como "${(personaName || 'ela').toString().trim()}".`,
    'A legenda deve combinar com o que est√° VIS√çVEL na imagem.',
    poseLine,
    hint ? `Contexto do pedido (use s√≥ se combinar com a imagem): ${String(hint).slice(0, 220)}` : '',
    'N√£o invente detalhes (ex.: flu√≠dos, atos espec√≠ficos, posi√ß√µes diferentes) se n√£o estiverem claramente vis√≠veis.',
    'Se houver nudez/sexo, descreva de forma objetiva e com linguagem leve (sem termos expl√≠citos).',
    'N√£o mencione idade. Assuma que √© adulto (18+).',
    'Responda somente com a legenda, sem listas e sem aspas.',
  ].join('\n')

  const res = await descreverImagemGrok({
    buffer,
    mimeType: mimeType || 'image/png',
    prompt,
    timeoutMs: 20000,
  })
  if (!res?.ok || !res?.text) return { ok: false, error: res?.error }
  const caption = clampText(res.text, 180)
  if (!caption) return { ok: false }
  return { ok: true, caption }
}

async function generateTtsAudio({ engines, chunks, xttsSamples, qwen3VoicePrompt, qwen3Samples }) {
  let lastError = null
  const startedAt = Date.now()
  console.log('[Audio][TTS] start', { engines, chunks: chunks.length })
  for (const engine of engines) {
    const t0 = Date.now()
    try {
      if (engine === 'qwen3') {
        if (!qwen3VoicePrompt) throw new Error('qwen3_voice_prompt_missing')
        if (chunks.length === 1) {
          console.log('[Audio][Qwen3] payload_text', { text: chunks[0] })
        } else {
          console.log('[Audio][Qwen3] payload_texts', { texts: chunks })
        }
        const gen = chunks.length === 1
          ? await audioQwen3Modal.generateSpeech(chunks[0], qwen3VoicePrompt, 'pt', qwen3Samples || null)
          : await audioQwen3Modal.generateSpeechBatch(chunks, qwen3VoicePrompt, 'pt', qwen3Samples || null)
        console.log('[Audio][TTS] success', { engine, ms: Date.now() - t0, totalMs: Date.now() - startedAt })
        return { ...gen, engine }
      }
      if (engine === 'xtts') {
        const samples = Array.isArray(xttsSamples) ? xttsSamples.filter(Boolean) : []
        if (!samples.length) throw new Error('xtts_sample_missing')
        console.log('[Audio][XTTS] sample', { count: samples.length, sampleBytes: samples[0]?.length || 0 })
        const gen = chunks.length === 1
          ? await audioModal.generateSpeech(chunks[0], samples, 'pt')
          : await audioModal.generateSpeechBatch(chunks, samples, 'pt')
        console.log('[Audio][TTS] success', { engine, ms: Date.now() - t0, totalMs: Date.now() - startedAt })
        return { ...gen, engine }
      }
    } catch (e) {
      lastError = e
      console.error('[Audio][TTS] engine_failed', { engine, ms: Date.now() - t0, error: e?.message || String(e) })
    }
  }
  if (lastError) throw lastError
  throw new Error('tts_failed')
}

function splitTextForAudio(text, opts = {}) {
  const raw = (text || '').toString().trim()
  if (!raw) return []
  const maxChars = Math.max(60, Number(opts.maxChars) || AUDIO_MAX_CHARS_PER_CHUNK)
  const maxChunks = Math.max(1, Number(opts.maxChunks) || AUDIO_MAX_CHUNKS)
  const minBreakAt = Math.max(30, Number(opts.minBreakAt) || Math.floor(maxChars * 0.55))
  const chunks = []
  let remaining = raw
  while (remaining.length > 0 && chunks.length < maxChunks) {
    if (remaining.length <= maxChars) {
      chunks.push(remaining)
      break
    }
    const cut = remaining.slice(0, maxChars)
    let idx = Math.max(cut.lastIndexOf('. '), cut.lastIndexOf('! '), cut.lastIndexOf('? '), cut.lastIndexOf('\n'), cut.lastIndexOf(', '))
    if (idx < minBreakAt) idx = cut.lastIndexOf(' ')
    if (idx < minBreakAt) idx = maxChars
    const part = remaining.slice(0, idx).trim()
    if (part) chunks.push(part)
    remaining = remaining.slice(idx).trim()
  }
  return chunks
}

function numeroPorExtensoPt(valor) {
  const n = Number(valor)
  if (!Number.isFinite(n)) return ''
  const inteiro = Math.floor(Math.abs(n))
  const unidades = ['zero', 'um', 'dois', 'tr√™s', 'quatro', 'cinco', 'seis', 'sete', 'oito', 'nove']
  const dezADezenove = ['dez', 'onze', 'doze', 'treze', 'quatorze', 'quinze', 'dezesseis', 'dezessete', 'dezoito', 'dezenove']
  const dezenas = ['', '', 'vinte', 'trinta', 'quarenta', 'cinquenta', 'sessenta', 'setenta', 'oitenta', 'noventa']
  const centenas = ['', 'cento', 'duzentos', 'trezentos', 'quatrocentos', 'quinhentos', 'seiscentos', 'setecentos', 'oitocentos', 'novecentos']
  if (inteiro === 100) return 'cem'
  if (inteiro < 10) return unidades[inteiro]
  if (inteiro < 20) return dezADezenove[inteiro - 10]
  if (inteiro < 100) {
    const d = Math.floor(inteiro / 10)
    const u = inteiro % 10
    return u ? `${dezenas[d]} e ${unidades[u]}` : dezenas[d]
  }
  if (inteiro < 1000) {
    const c = Math.floor(inteiro / 100)
    const r = inteiro % 100
    if (!r) return centenas[c]
    const resto = numeroPorExtensoPt(r)
    return resto ? `${centenas[c]} e ${resto}` : centenas[c]
  }
  return inteiro.toString()
}

function expandirNumerosPt(texto) {
  return (texto || '').toString().replace(/\b\d{1,3}\b/g, (m, offset, full) => {
    const prev = full[offset - 1] || ''
    const next = full[offset + m.length] || ''
    if (prev === ':' || next === ':' || prev === '/' || next === '/' || next === '%' || next === '¬∫' || next === '¬∞') return m
    const ext = numeroPorExtensoPt(parseInt(m, 10))
    return ext || m
  })
}

function normalizeTextForTTS(input, opts = {}) {
  let t = (input || '').toString()
  const preserveCueTags = !!opts.preserveCueTags
  if (/(ajustando:|falando certo:|n√£o,\s*claro:|n√£o,\s*claro\b|deixa eu corrigir:|exato\s+assim\b)/i.test(t)) {
    t = t.split(/(?:ajustando:|falando certo:|deixa eu corrigir:|n√£o,\s*claro:|n√£o,\s*claro\b|exato\s+assim\b)/i).pop() || t
  }
  t = t.replace(/\[SEND_PHOTO:\s*.+?\]/gi, ' ')
  t = t.replace(/\[[^\]]+\]/g, (full) => {
    if (!preserveCueTags) return ' '
    const inner = full.slice(1, -1).toLowerCase()
    if (/(sussurr|ofeg|gemend|suspi|risad|rindo|rouc|choram|choro)/.test(inner)) return full
    return ' '
  })
  t = t.replace(/\*[^*]{1,220}\*/g, ' ')
  t = t.replace(/\([^)]{0,420}\)/g, ' ')
  t = t.replace(/["‚Äú‚Äù]/g, '')
  t = t.replace(/\p{Extended_Pictographic}+/gu, ' ')
  t = t.replace(/(^|[.!?‚Ä¶]\s*)[^.!?‚Ä¶]*\b(√°udio|audio)\b[^.!?‚Ä¶]*[.!?‚Ä¶]?/gi, ' ')
  t = t.replace(/(^|[.!?‚Ä¶]\s*)[^.!?‚Ä¶]*(exato\s+)?como\s+(voc√™\s+)?pediu[^.!?‚Ä¶]*[.!?‚Ä¶]?/gi, ' ')
  t = t.replace(/(^|[.!?‚Ä¶]\s*)[^.!?‚Ä¶]*(do\s+)?jeito\s+que\s+(voc√™\s+)?pediu[^.!?‚Ä¶]*[.!?‚Ä¶]?/gi, ' ')
  t = t.replace(/(^|[.!?‚Ä¶]\s*)[^.!?‚Ä¶]*quer\s+ajuste[^.!?‚Ä¶]*[.!?‚Ä¶]?/gi, ' ')
  t = t.replace(/\b(pra|para)\s+mim\s*,?\s+seu\s+dom\b/gi, 'pro meu Dom')
  t = t.replace(/\b(pra|para)\s+mim\s+seu\s+dom\b/gi, 'pro meu Dom')
  t = t.replace(/(^|[.!?‚Ä¶]\s*)(grava|grave)\s+(pra|para)\s+(mandar|enviar|ela)[^.!?‚Ä¶]*[.!?‚Ä¶]?/gi, ' ')
  t = t.replace(/(^|[.!?‚Ä¶]\s*)(topou|topa)\s+gravar[^.!?‚Ä¶]*[.!?‚Ä¶]?/gi, ' ')
  t = t.replace(/(^|[.!?‚Ä¶]\s*)(manda|envia)\s+(isso|esse|essa|este)\b[^.!?‚Ä¶]*[.!?‚Ä¶]?/gi, ' ')
  t = t.replace(/(^|[.!?‚Ä¶]\s*)(vou|vamo|vamos)\s+(te\s+)?mandar\s+(um\s+)?(√°udio|audio)[^.!?‚Ä¶]*[.!?‚Ä¶]?/gi, ' ')
  t = t.replace(/(^|[.!?‚Ä¶]\s*)(perfeito)\s+assim[^.!?‚Ä¶]*[.!?‚Ä¶]?/gi, ' ')
  t = t.replace(/(^|[.!?‚Ä¶]\s*)(meu\s+dom)\s+aprova[^.!?‚Ä¶]*[.!?‚Ä¶]?/gi, ' ')
  t = t.replace(/(^|[.!?‚Ä¶]\s*)(satisfeito|gostou|aprovou)\s+(meu|minha)\s+(dom|rei|amor)[^.!?‚Ä¶]*[.!?‚Ä¶]?/gi, ' ')
  t = t.replace(/(^|[.!?‚Ä¶]\s*)(n√£o,\s*)?(espera|espere)[^.!?‚Ä¶]*([.!?‚Ä¶]|$)/gi, ' ')
  t = t.replace(/(^|[.!?‚Ä¶]\s*)\(?nota:[^.!?‚Ä¶]*([.!?‚Ä¶]|$)/gi, ' ')
  t = t.replace(/(^|[.!?‚Ä¶]\s*)(n√£o,\s*)?claro[^.!?‚Ä¶]*([.!?‚Ä¶]|$)/gi, ' ')
  t = t.replace(/(^|[.!?‚Ä¶]\s*)deixa\s+eu\s+corrigir[^.!?‚Ä¶]*([.!?‚Ä¶]|$)/gi, ' ')
  const replacements = [
    [/\bbb\b/gi, 'beb√™'],
    [/\bvc\b/gi, 'voc√™'],
    [/\bpq\b/gi, 'porque'],
    [/\bvoce\b/gi, 'voc√™'],
    [/\bvoces\b/gi, 'voc√™s'],
    [/\bta\b/gi, 't√°'],
    [/\btava\b/gi, 'tava'],
    [/\bto\b/gi, 't√¥'],
    [/\bnao\b/gi, 'n√£o'],
    [/\btb\b/gi, 'tamb√©m'],
    [/\btd\b/gi, 'tudo'],
    [/\bvdd\b/gi, 'verdade'],
    [/\bblz\b/gi, 'beleza'],
  ]
  for (const [re, val] of replacements) {
    t = t.replace(re, val)
  }
  t = expandirNumerosPt(t)
  t = t.replace(/\s+/g, ' ').trim()
  return t
}

function extractAudioCuesFromText(input, opts = {}) {
  const raw = (input || '').toString()
  if (!raw) return { text: raw, cuePrompt: '' }
  const preserveTokens = !!opts.preserveTokens
  const cues = new Set()
  const cleaned = raw.replace(/\[([^\]]{1,80})\]/g, (full, inner) => {
    const v = (inner || '').toString().trim()
    if (!v) return full
    const lower = v.toLowerCase()
    if (lower.startsWith('send_photo')) return full
    if (/foto\s*:/.test(lower)) return full
    if (/(sussurr|ofeg|gemend|suspi|risad|rindo|rouc|choram|choro)/.test(lower)) {
      if (lower.includes('sussurr')) cues.add('sussurrando')
      if (lower.includes('ofeg')) cues.add('ofegante')
      if (lower.includes('gemend')) cues.add('gemendo')
      if (lower.includes('suspi')) cues.add('suspirando')
      if (lower.includes('risad') || lower.includes('rindo')) cues.add('risadinha')
      if (lower.includes('rouc')) cues.add('voz rouca')
      if (lower.includes('choram') || lower.includes('choro')) cues.add('choramingando')
      return preserveTokens ? full : ' '
    }
    return full
  })
  const list = Array.from(cues)
  if (!list.length) return { text: cleaned, cuePrompt: '' }
  const parts = []
  if (list.includes('sussurrando')) parts.push('Fale em estilo ASMR: sussurro bem baixinho, bem pr√≥ximo, com respira√ß√£o suave e pausas curtas.')
  if (list.includes('ofegante')) parts.push('Fale ofegante, com respira√ß√£o aud√≠vel e pausas curtas para ar.')
  if (list.includes('gemendo')) parts.push('Inclua gemidos aud√≠veis e prolongados, intensidade alta, com varia√ß√£o natural.')
  if (list.includes('suspirando')) parts.push('Inclua suspiros suaves e naturais.')
  if (list.includes('risadinha')) parts.push('Inclua risadinhas leves e sedutoras, pontuais.')
  if (list.includes('voz rouca')) parts.push('Use voz um pouco rouca e mais grave.')
  if (list.includes('choramingando')) parts.push('Use voz tr√™mula e choramingada, com emo√ß√£o.')
  const cuePrompt = parts.join(' ')
  return { text: cleaned, cuePrompt }
}

function isMoanOnlyRequest(inputText) {
  const t = (inputText || '').toString().toLowerCase()
  if (!t) return false
  if (/(sem\s+falar|sem\s+palavras|s[o√≥]\s+gemid|somente\s+gemid|apenas\s+gemid|s[o√≥]\s+gemendo|apenas\s+gemendo|somente\s+gemendo)/.test(t)) return true
  if (/\bassim\s*[:\-‚Äì‚Äî]/.test(t)) return false
  if (/(falando|fale|fala\s+que|diz|diga|me\s+chama|chama\s+meu\s+nome)/.test(t)) return false
  const asksAudio = /(√°udio|audio|voice note|nota de voz)/.test(t)
  const hasMoan = /(gemid|gemend|gozand|orgasm|cl[i√≠]max)/.test(t)
  const short = t.split(/\s+/).filter(Boolean).length <= 12
  if (asksAudio && hasMoan) return true
  if (short && hasMoan) return true
  return false
}

function buildMoanText(inputText) {
  const t = (inputText || '').toString().toLowerCase()
  const intense = /(exagerad|bem\s+gostos|gozand|orgasm|cl[i√≠]max)/.test(t)
  if (intense) return 'Mmm‚Ä¶ ahh‚Ä¶ ahh‚Ä¶ mmm‚Ä¶ ai‚Ä¶ ahh‚Ä¶ mais‚Ä¶ mmm‚Ä¶ ahhh‚Ä¶ ai‚Ä¶ ahh‚Ä¶ mmm‚Ä¶'
  return 'Mmm‚Ä¶ ahh‚Ä¶ mmm‚Ä¶ ahh‚Ä¶ ai‚Ä¶ mmm‚Ä¶'
}

function tuneQwen3VoicePromptForCues(basePrompt, cuePrompt) {
  const base = (basePrompt || '').toString()
  const cue = (cuePrompt || '').toString()
  if (!base) return cue.trim()
  const wantsExpressive = /(asmr|sussurr|ofeg|gemid|gemend|gozand|orgasm|cl[i√≠]max)/i.test(cue)
  if (!wantsExpressive) return base
  let tuned = base
  tuned = tuned.replace(/sem\s+gemidos\s+exagerados\.?/gi, '').trim()
  tuned = tuned.replace(/leitura\s+fiel\s+ao\s+texto\.?/gi, '').trim()
  tuned = tuned.replace(/\s+/g, ' ').trim()
  const add = 'Evite leitura rob√≥tica. Soe crua, √≠ntima e espont√¢nea, com varia√ß√£o de volume, respira√ß√£o aud√≠vel e pausas naturais.'
  return `${tuned} ${add}`.trim()
}

function extractAudioCuesFromUserRequest(input) {
  const t = (input || '').toString().toLowerCase()
  if (!t) return ''
  const cues = new Set()
  if (/(sussurr|sussurro)/.test(t)) cues.add('sussurrando')
  if (/(ofeg|ofegante|respira√ß√£o pesada|respiracao pesada)/.test(t)) cues.add('ofegante')
  if (/(gemend|gemido|gemidos)/.test(t)) cues.add('gemendo')
  if (/(gozand|orgasm|cl[i√≠]max)/.test(t)) cues.add('orgasmo')
  if (/(suspir|suspiro|suspiros)/.test(t)) cues.add('suspirando')
  if (/(risad|risadinha|rindo|riso baixo)/.test(t)) cues.add('risadinha')
  if (/(rouc|voz rouca)/.test(t)) cues.add('voz rouca')
  if (/(choram|chorando|choro|choraming)/.test(t)) cues.add('choramingando')
  const parts = []
  if (cues.has('sussurrando')) parts.push('Fale em estilo ASMR: sussurro bem baixinho, bem pr√≥ximo, com respira√ß√£o suave e pausas curtas.')
  if (cues.has('ofegante')) parts.push('Fale ofegante, com respira√ß√£o aud√≠vel e pausas curtas para ar.')
  if (cues.has('gemendo')) parts.push('Inclua gemidos aud√≠veis e prolongados; pode ser exagerado, sempre natural.')
  if (cues.has('orgasmo')) parts.push('Fa√ßa um pico de cl√≠max: gemidos bem mais intensos, voz tremendo, respira√ß√£o acelerada.')
  if (cues.has('suspirando')) parts.push('Inclua suspiros suaves e naturais.')
  if (cues.has('risadinha')) parts.push('Inclua risadinhas leves e sedutoras, pontuais.')
  if (cues.has('voz rouca')) parts.push('Use voz um pouco rouca e mais grave.')
  if (cues.has('choramingando')) parts.push('Use voz tr√™mula e choramingada, com emo√ß√£o.')
  return parts.join(' ').trim()
}

function extractAudioCueTokensFromUserRequest(input) {
  const t = (input || '').toString().toLowerCase()
  if (!t) return []
  const tokens = []
  const push = (v) => { if (v && !tokens.includes(v)) tokens.push(v) }
  if (/(sussurr|sussurro)/.test(t)) push('[sussurrando]')
  if (/(ofeg|ofegante|respira√ß√£o pesada|respiracao pesada)/.test(t)) push('[ofegante]')
  if (/(gemend|gemido|gemidos)/.test(t)) push('[gemendo]')
  if (/(suspir|suspiro|suspiros)/.test(t)) push('[suspirando]')
  if (/(risad|risadinha|rindo|riso baixo)/.test(t)) push('[risadinha]')
  if (/(rouc|voz rouca)/.test(t)) push('[voz rouca]')
  if (/(choram|chorando|choro|choraming)/.test(t)) push('[choramingando]')
  return tokens.slice(0, 2)
}

function userWantsCueTagsInText(input) {
  const t = (input || '').toString().toLowerCase()
  return /\b(tag|tags|colchete|colchetes|marcador|marcadores)\b/.test(t)
    || /no\s+meio\s+do\s+texto/.test(t)
    || /inclu(i|a)\s+.*\b(tag|tags|colchete|colchetes|marcador|marcadores)\b/.test(t)
}

function injectCueTokensIntoText(text, tokens) {
  let out = (text || '').toString()
  const arr = Array.isArray(tokens) ? tokens.filter(Boolean).slice(0, 2) : []
  if (!out.trim() || !arr.length) return out
  const firstComma = out.indexOf(',')
  let i1 = firstComma >= 0 ? firstComma + 1 : Math.min(Math.max(12, Math.floor(out.length * 0.25)), out.length)
  out = `${out.slice(0, i1)} ${arr[0]} ${out.slice(i1)}`.replace(/\s+/g, ' ').trim()
  if (arr.length >= 2) {
    let i2 = Math.min(Math.max(20, Math.floor(out.length * 0.6)), out.length)
    out = `${out.slice(0, i2)} ${arr[1]} ${out.slice(i2)}`.replace(/\s+/g, ' ').trim()
  }
  return out
}

function mergeCuePrompts(a, b) {
  const x = (a || '').toString().trim()
  const y = (b || '').toString().trim()
  if (!x) return y
  if (!y) return x
  if (x === y) return x
  return `${x} ${y}`.trim()
}

function postProcessTextForAudio(spokenText, userText) {
  const original = (spokenText || '').toString().trim()
  if (!original) return original
  let t = original
  t = t.replace(/(^|[.!?‚Ä¶]\s*)[^.!?‚Ä¶]*\b(√°udio|audio)\b[^.!?‚Ä¶]*[.!?‚Ä¶]?/gi, ' ')
  const userAskedQuestion = /\?/.test((userText || '').toString())
  if (!userAskedQuestion) {
    t = t.replace(/[^.!?‚Ä¶]*\?+/g, ' ')
  }
  t = t.replace(/\s+/g, ' ').trim()
  return t || original
}

function isRefusalAnswer(text) {
  const s = (text || '').toString().trim().toLowerCase()
  if (!s) return false
  if (s === 'n√£o' || s === 'n√£o.' || s.startsWith('n√£o,') || s.startsWith('n√£o.')) return true
  if (s.includes('n√£o posso') || s.includes('n√£o vou') || s.includes('n√£o consigo')) return true
  if (s.includes('vamos brincar de outro jeito')) return true
  return false
}

function buildScriptFallbackText(scriptReq) {
  if (!scriptReq) return ''
  const target = (scriptReq.target || '').toString().trim()
  let msg = (scriptReq.message || '').toString().trim()
  if (!msg) return ''
  if (target && msg.toLowerCase().startsWith(target.toLowerCase())) return msg
  if (target) return `Oi ${target}, ${msg}`
  return msg
}

function normalizeTextForBark(input, opts = {}) {
  let t = normalizeTextForTTS(input, opts)
  if (!t) return t
  t = t.replace(/\s*([.!?‚Ä¶])\s*/g, '$1 ')
  t = t.replace(/\s*,\s*/g, ', ')
  t = t.replace(/\s*;\s*/g, '; ')
  t = t.replace(/\s*:\s*/g, ': ')
  t = t.replace(/\s+-\s+/g, '. ')
  t = t.replace(/\.{3,}/g, '...')
  t = t.replace(/([!?]){2,}/g, '$1')
  t = t.replace(/\s+/g, ' ').trim()
  if (!/[.!?]$/.test(t)) t = `${t}.`
  return t
}

function userWantsPhoto(inputText) {
  const t = (inputText || '').toString().toLowerCase()
  const wants = /\b(manda|envia|me manda|mostra|me mostra|gera|quero|pode mandar)\b/.test(t)
  const target = /\b(foto|imagem|selfie|nude|nudes|bunda|peito|peitos)\b/.test(t)
  return wants && target
}

function isPotentialMinorContent(inputText) {
  const t = (inputText || '').toString().toLowerCase()
  return /\b(menor|menina|garotinha|garota|novinha|ninfeta|adolescente|colegial|schoolgirl|teen)\b/.test(t)
    || /\b(1[0-7])\s*anos\b/.test(t)
}

function shouldForceAudioByRequest(inputText) {
  const t = (inputText || '').toString().toLowerCase()
  return /(manda|envia|me manda|responde).*(√°udio|audio)/.test(t)
    || /(por|em)\s+(√°udio|audio)/.test(t)
    || /(voice note|nota de voz)/.test(t)
}

function extractScriptRequest(inputText) {
  const raw = (inputText || '').toString().trim()
  if (!raw) return null
  const m = raw.match(/\b(fala|fale|diz|diga|manda|envia|responde|grava)\b[\s\S]{0,80}?\b(pra|para)\s+([A-Za-z√Ä-√∫][A-Za-z√Ä-√∫'\-]{1,40})\b/i)
  if (!m) return null
  const target = (m[3] || '').trim()
  const targetLower = target.toLowerCase()
  const invalidTargets = new Set([
    'mim', 'me', 'eu', 'minha', 'meu',
    'voc√™', 'voce', 'vc',
    'ele', 'ela', 'dele', 'dela',
    'a', 'o', 'os', 'as',
    'dom', 'rei', 'amor', 'vida',
  ])
  if (invalidTargets.has(targetLower)) return null
  const after = raw.slice((m.index || 0) + m[0].length).trim()
  const lowerAfter = after.toLowerCase()
  const assimMatch = lowerAfter.match(/\bassim\s*[:\-‚Äì‚Äî]\s*/i)
  const idxAssim = assimMatch ? lowerAfter.indexOf(assimMatch[0]) : -1
  const idxQue = lowerAfter.indexOf('que ')
  let explicitScript = false
  let message = after
  if (idxAssim >= 0) {
    message = after.slice(idxAssim + (assimMatch?.[0]?.length || 0))
    explicitScript = true
  } else if (idxQue >= 0) {
    message = after.slice(idxQue + 4)
  }
  message = message.replace(/^[:\-‚Äì‚Äî\s]+/, '').trim()
  const mentionsDom = /\b(dom|meu\s+dom|seu\s+dom)\b/i.test(raw) || /\b(seu)\s+dom\b/i.test(message)
  if (!message) return { target, message: '' }
  return { target, message, mentionsDom, explicitScript }
}

export async function handleConversaAgente(ctx) {
  const { prisma, personaReady, state, typed, text, conv, persona, user, sendId, phone, sendWhatsAppText, sendWhatsAppButtons, sendWhatsAppAudioLink, sendWhatsAppChatState, maps } = ctx

  const upsellTrialMessages = [
    "Amor, queria muito te mandar essa foto, mas meu app diz que voc√™ precisa ser assinante VIP pra ver... Que tal assinar agora pra gente n√£o ter limites? üòà",
    "Poxa vida, tentei te enviar mas bloqueou... Parece que √© s√≥ para assinantes. Vem ser meu VIP? üëá",
  ]
  const upsellLimitMessages = [
    "Vida, acabei estourando meu plano de dados por hoje pra mandar fotos... üôà Me ajuda com um pacote extra pra eu continuar te mandando? T√¥ doida pra te mostrar...",
    "Amor, gastei todos os meus cr√©ditos de foto por hoje... ü•∫ Mas se voc√™ me der um presentinho, eu consigo te mandar agora mesmo!",
    "Nossa, bloqueou aqui... Diz que atingimos o limite de hoje. Que tal um pacote extra pra gente n√£o parar? üòà"
  ]
  const getRandom = (arr) => arr[Math.floor(Math.random() * arr.length)]

  if (!personaReady || state) return false

  if (isPotentialMinorContent(text)) {
    const replyText = 'N√£o posso continuar com conte√∫do que envolva menor de idade. Se quiser, posso falar de uma fantasia consensual entre adultos.'
    await salvarSaidaEEnviar({
      prisma,
      store: 'message',
      conversationId: conv.id,
      userId: user.id,
      personaId: persona.id,
      content: replyText,
      enviar: () => sendWhatsAppText(sendId, phone, replyText),
    })
    return true
  }

  const responseModePre = (persona?.responseMode || 'text').toString()
  const inboundIsAudioPre = ctx.msgType === 'audio'
  const forceAudioPre = shouldForceAudioByRequest(text)
  const willNeedAudioResponse =
    responseModePre === 'audio'
    || ((responseModePre === 'mirror' || responseModePre === 'both') && inboundIsAudioPre)
    || forceAudioPre
  const wantsPhoto = userWantsPhoto(text)
  const scriptReq = willNeedAudioResponse ? extractScriptRequest(text) : null
  const userCuePrompt = willNeedAudioResponse ? extractAudioCuesFromUserRequest(text) : ''
  const cueTagsMode = (process.env.AUDIO_CUE_TAGS_MODE || 'voice_prompt').toString().trim().toLowerCase()
  const passThroughCueTags = willNeedAudioResponse && cueTagsMode === 'pass_through'
  const shouldInsertCueTags = willNeedAudioResponse && ((process.env.AUDIO_INSERT_CUE_TAGS_IN_TEXT || '0').toString().trim() === '1')
  const userCueTokens = willNeedAudioResponse ? extractAudioCueTokensFromUserRequest(text) : []
  const moanOnly = willNeedAudioResponse && !scriptReq && isMoanOnlyRequest(text)

  const t = text.trim().toLowerCase()
  const isGreeting = !!t && (
    t === 'oi' || t === 'ol√°' || t === 'ola' ||
    t.includes('oi crush') ||
    t.includes('quer namorar comigo') ||
    t.includes('quer ser minha crush')
  )

  if ((personaReady && isGreeting) || (personaReady && !maps.onboarding.get(user.id))) {
    const basePrompt = composeSystemPrompt(persona)
    const audioExpressiveness = `\n\nGUIA (√ÅUDIO):\n- Para soar humano, use pausas e varia√ß√£o de ritmo no pr√≥prio texto.\n- Use interjei√ß√µes curtas e naturais (1‚Äì3 no m√°ximo), como: "Ah...", "Hmm...", "Ufa...", "Ei...", "Hehe...".\n- Se o usu√°rio pedir voz sussurrada/ofegante/gemendo, reflita isso com pontua√ß√£o e algumas onomatopeias discretas (ex.: "mmm", "ahh"), sem exagerar.\n- Use retic√™ncias e v√≠rgulas para marcar pausas e respira√ß√£o.\n- N√£o use marcadores em colchetes no texto falado (eles viram fala literal no TTS).`.trim()
    const systemPrompt = willNeedAudioResponse
      ? (wantsPhoto
        ? `${basePrompt}${audioExpressiveness}\n\nIMPORTANTE:\n- O usu√°rio pediu foto: comece com [SEND_PHOTO: english description].\n- Depois da tag, escreva apenas o texto falado.\n- Use ortografia PT-BR correta (acentos e cedilha). Evite escrever \"voce\", \"voces\", \"nao\", \"ta\".\n- N√£o use a palavra \"√°udio\"/\"audio\" no texto falado.\n- No texto falado, n√£o use ponto de interroga√ß√£o (?).\n- No texto falado: proibido a√ß√µes/ambienta√ß√£o entre asteriscos, emojis, aspas e colchetes.\n- Evite abrevia√ß√µes (ex.: bb, vc, pq). Escreva por extenso.\n- N√∫meros sempre por extenso.\n- Use pontua√ß√£o natural para soar humano: frases curtas, v√≠rgulas e pausas.`
        : `${basePrompt}${audioExpressiveness}\n\nIMPORTANTE:\n- Retorne apenas o texto falado.\n- Responda com UMA √∫nica vers√£o final do texto (um √∫nico bloco). N√£o d√™ op√ß√µes/varia√ß√µes.\n- Nunca responda com apenas uma palavra (ex.: \"vai\"). Gere ao menos 1 frase completa com 10+ palavras.\n- Use ortografia PT-BR correta (acentos e cedilha). Evite escrever \"voce\", \"voces\", \"nao\", \"ta\".\n- Proibido: a√ß√µes/ambienta√ß√£o entre asteriscos, emojis, aspas, colchetes, par√™nteses e tags (incluindo [SEND_PHOTO: ...]).\n- N√£o mencione √°udio/grava√ß√£o nem pe√ßa confirma√ß√£o para gravar (ex.: \"vou gravar\", \"grava pra mandar\", \"topa gravar?\").\n- N√£o use a palavra \"√°udio\"/\"audio\" no texto falado.\n- No texto falado, n√£o use ponto de interroga√ß√£o (?).\n- Atenda diretamente o pedido: n√£o termine com pergunta de valida√ß√£o/checagem (ex.: \"satisfeito?\", \"gostou?\", \"meu Dom aprovou?\") a menos que o usu√°rio tenha feito uma pergunta.\n- N√£o fa√ßa auto-corre√ß√£o no texto e n√£o pense em voz alta (ex.: \"n√£o\", \"n√£o, claro\", \"espera\", \"deixa eu corrigir\", \"falando certo\", \"ajustando\", \"exato\").\n- N√£o inclua notas/explica√ß√µes/metacoment√°rios (ex.: \"nota:\", \"observa√ß√£o:\"). Se houver ambiguidade, escolha a interpreta√ß√£o mais prov√°vel e escreva apenas a vers√£o final.\n- Evite abrevia√ß√µes (ex.: bb, vc, pq). Escreva por extenso.\n- N√∫meros sempre por extenso.\n- Use pontua√ß√£o natural para soar humano: frases curtas, v√≠rgulas e pausas.\n- Se voc√™ escrever qualquer coisa proibida, reescreva e devolva somente o texto falado.`
      )
      : `${basePrompt}\n\nIMPORTANTE:\n- S√≥ use [SEND_PHOTO: ...] se o usu√°rio pedir foto explicitamente.`
    const prev = (conv.xaiLastResponseId || '').toString().trim()
    const convCacheId = (conv.xaiConvCacheId || '').toString().trim()

    // INJE√á√ÉO DE REGRA DE IMAGEM: For√ßa o LLM a lembrar da regra de imagem em ingl√™s a cada turno.
    // Isso √© invis√≠vel para o usu√°rio final no WhatsApp, mas vis√≠vel para o LLM.
    const imageRule = wantsPhoto
      ? "\n\n[SYSTEM: IMPORTANT: If you generate a photo tag [SEND_PHOTO: description], the description MUST BE IN ENGLISH ONLY. Translate any visual details from Portuguese to English inside the tag. Example: [SEND_PHOTO: selfie of an adult woman, blonde hair, smiling]. Do NOT use Portuguese inside the tag.]"
      : ""
    const scriptRule = scriptReq
      ? `\n\nIMPORTANTE (CONTEXTO):\n- Voc√™ √© ${persona.name}. O usu√°rio √© o seu Dom.\n- O usu√°rio quer que voc√™ fale diretamente com ${scriptReq.target} (terceira pessoa).\n- Escreva a mensagem como se voc√™ estivesse falando com ${scriptReq.target} agora.\n- O Dom N√ÉO √© voc√™. Voc√™ n√£o diz \"pra mim, seu Dom\".\n- Quando o pedido mencionar \"seu Dom\", entenda que √© o usu√°rio (o Dom) e voc√™ deve falar \"meu Dom\" ou \"o meu Dom\".\n- N√£o se dirija ao usu√°rio, n√£o diga \"como pediu\"/\"exato como pediu\" e n√£o finalize pedindo ajuste.`
      : ""
    const userForModel = scriptReq
      ? `Tarefa: escreva a fala de ${persona.name} para ${scriptReq.target}.\nRegras: n√£o fale com o usu√°rio, n√£o confirme pedido, n√£o use a palavra √°udio, n√£o use interroga√ß√£o.\n${scriptReq.mentionsDom ? 'Contexto: o usu√°rio √© o Dom; ao mencionar o Dom, use \"meu Dom\" (nunca \"pra mim, seu Dom\").\n' : ''}${scriptReq.explicitScript ? 'O usu√°rio forneceu um texto pronto. Repita o texto abaixo exatamente, sem resumir, sem parafrasear e sem remover palavras.\nTexto:\n' : 'Use o conte√∫do abaixo como base. N√£o resuma e n√£o corte trechos.\nConte√∫do:\n'}${scriptReq.message || text}`.trim()
      : text
    
    const chat = prev
      ? [{ role: 'user', content: userForModel + imageRule }]
      : [...(await buildLLMMessages(prisma, conv.id, systemPrompt + scriptRule)), { role: 'user', content: userForModel + imageRule }]
    
    const gen = await generateWithGrok(chat, { useStore: true, previousResponseId: prev || undefined, convCacheId: convCacheId || undefined })
    if (gen?.blocked) {
      const detail = (gen?.errorMessage || '').toString()
      const isCsam = detail.toLowerCase().includes('safety_check_type_csam')
      const replyText = isCsam
        ? 'N√£o posso responder esse pedido desse jeito. Se quiser, descreva uma fantasia consensual entre adultos (18+), sem termos como ‚Äúnovinha/menina/colegial‚Äù, que eu respondo.'
        : WHATSAPP_FALLBACK_BLOQUEIO_CONTEUDO
      try {
        await prisma.conversation.update({
          where: { id: conv.id },
          data: { xaiLastResponseId: null, xaiLastResponseAt: null, xaiConvCacheId: crypto.randomUUID() }
        })
      } catch {}
      await salvarSaidaEEnviar({
        prisma,
        store: 'message',
        conversationId: conv.id,
        userId: user.id,
        personaId: persona.id,
        content: replyText,
        enviar: () => sendWhatsAppText(sendId, phone, replyText),
      })
      return true
    }
    if (gen?.responseId) {
      try { await prisma.conversation.update({ where: { id: conv.id }, data: { xaiLastResponseId: gen.responseId, xaiLastResponseAt: new Date() } }) } catch {}
    }
    
    let replyTextRaw = gen.ok && gen.content ? gen.content : WHATSAPP_FALLBACK_ERRO_GERACAO
    if (willNeedAudioResponse && scriptReq && scriptReq.explicitScript && scriptReq.message) {
      replyTextRaw = scriptReq.message
    }
    if (willNeedAudioResponse && scriptReq && scriptReq.message && isRefusalAnswer(replyTextRaw)) {
      replyTextRaw = buildScriptFallbackText(scriptReq) || replyTextRaw
    }
    console.log('[ConversaAgente] Resposta LLM:', replyTextRaw)

    const photoMatch = replyTextRaw.match(/\[SEND_PHOTO:\s*(.+?)\]/i) || 
                       replyTextRaw.match(/\(Foto:\s*(.+?)\)/i) ||
                       replyTextRaw.match(/\*\(Foto:\s*(.+?)\)\*/i) ||
                       replyTextRaw.match(/\*Foto:\s*(.+?)\*/i)

    let captionText = replyTextRaw
    if (photoMatch) {
      captionText = captionText.replace(/\[SEND_PHOTO:\s*(.+?)\]/gi, '')
                               .replace(/\(Foto:\s*(.+?)\)/gi, '')
                               .replace(/\*\(Foto:\s*(.+?)\)\*/gi, '')
                               .replace(/\*Foto:\s*(.+?)\*/gi, '')
                               .trim()
    }
    let replyText = captionText
    let qwen3CuePrompt = ''
    if (willNeedAudioResponse) {
      if (moanOnly) {
        replyText = buildMoanText(text)
      }
      if (shouldInsertCueTags && userCueTokens.length) {
        replyText = injectCueTokensIntoText(replyText, userCueTokens)
        const positions = userCueTokens.map((tok) => ({ tok, idx: replyText.indexOf(tok) }))
        console.log('[Audio][Cue] inserted', { personaId: persona.id, count: userCueTokens.length, tokens: userCueTokens, positions })
        console.log('[ConversaAgente] cues_in_text', { personaId: persona.id, preview: replyText.slice(0, 260) })
      }
      const cues = extractAudioCuesFromText(replyText, { preserveTokens: passThroughCueTags })
      qwen3CuePrompt = mergeCuePrompts(userCuePrompt, (cues.cuePrompt || '').toString().trim())
      replyText = normalizeTextForTTS(cues.text, { preserveCueTags: passThroughCueTags }) || replyText
      replyText = postProcessTextForAudio(replyText, text) || replyText
      if (!scriptReq?.explicitScript) {
        const words = replyText.split(/\s+/).filter(Boolean)
        if (words.length < 4) {
          const wantsMoan = /(gemid|gemend|gemendo)/i.test(userCuePrompt)
          const wantsWhisper = /(asmr|sussurr)/i.test(userCuePrompt)
          replyText = wantsWhisper
            ? 'Ei‚Ä¶ vem c√°, bem pertinho. Fala comigo baixinho‚Ä¶ mmm‚Ä¶'
            : wantsMoan
              ? 'Ah‚Ä¶ mmm‚Ä¶ vem c√°. Eu quero te ouvir bem de perto‚Ä¶ ahh‚Ä¶'
              : 'Ei‚Ä¶ vem c√°. Fala comigo‚Ä¶'
        }
      }
      console.log('[ConversaAgente] audio_prep', { personaId: persona.id, cueTagsMode, passThroughCueTags, cueLen: (qwen3CuePrompt || '').length, cue: (qwen3CuePrompt || '').slice(0, 140), preview: replyText.slice(0, 260) })
    }

    let shouldSendText = true

    if (photoMatch) {
      console.log('[ConversaAgente] Detectado pedido de foto:', photoMatch[1])
      // Verifica√ß√£o de Recupera√ß√£o: Se a persona n√£o tem avatar, assumimos que √© uma falha de onboarding
      // e permitimos a gera√ß√£o gratuita para corrigir o problema.
      const hasAvatar = persona.avatar && (persona.avatar.startsWith('http') || persona.avatar.startsWith('https'))
      const isRecoveryFlow = !hasAvatar

      let allowance = { allowed: true }
      if (!isRecoveryFlow) {
        allowance = await checkImageAllowance(prisma, user.id)
      }
      console.log('[ConversaAgente] Allowance check:', allowance, 'Recovery:', isRecoveryFlow)
      
      if (!allowance.allowed) {
        shouldSendText = false
        let upsellText = ""
        let buttons = []
        
        if (allowance.reason === 'trial') {
          upsellText = getRandom(upsellTrialMessages)
          buttons = [
            { id: 'upgrade_conhecer_planos', title: 'VER PLANOS' },
            { id: 'upgrade_agora_nao', title: 'AGORA N√ÉO' }
          ]
        } else {
          upsellText = getRandom(upsellLimitMessages)
          buttons = [
            { id: 'billing_pacote_fotos', title: 'COMPRAR FOTOS' },
            { id: 'billing_agora_nao', title: 'AGORA N√ÉO' }
          ]
        }

        await salvarSaidaEEnviar({
          prisma,
          store: 'message',
          conversationId: conv.id,
          userId: user.id,
          personaId: persona.id,
          content: upsellText,
          enviar: () => sendWhatsAppButtons(sendId, phone, upsellText, buttons)
        })
        return true
      } else {
          // Gera e envia imagem usando Helper unificado
          shouldSendText = willNeedAudioResponse

          // Removido mensagem de espera (buffer) conforme solicitado, pois a gera√ß√£o j√° demora.
          
          const traits = getPersonaPhysicalTraits(persona.prompt)
          let { prompt: finalPrompt, negative: negativePrompt, poseType } = resolveImagePrompt(text, photoMatch[1], traits)
          let refs = []
          let totalAvailableRefs = 0
          try {
            const envBucketValRefs = (process.env.SUPABASE_BUCKET_FOTOS_REFS || 'crushzap/images/refs-images').toString()
            let bucketNameRefs = envBucketValRefs
            let pathPrefixRefs = ''
            if (envBucketValRefs.includes('/')) {
              const parts = envBucketValRefs.split('/')
              bucketNameRefs = parts[0]
              pathPrefixRefs = parts.slice(1).join('/')
            }
            const list = await listarPublicUrlsSupabase({ prefix: `${pathPrefixRefs}/${persona.id}`, bucketName: bucketNameRefs, limit: 50 })
            if (list.ok) {
              const items = Array.isArray(list.items) ? list.items : []
              totalAvailableRefs = items.length
              const names = items
                .map(it => ({ name: String(it.name).toLowerCase(), url: it.publicUrl }))
                .sort((a, b) => b.name.localeCompare(a.name))
              const selectRefsByPoseType = (poseTypeValue) => {
                const pick = []
                const pushBy = (test) => {
                  names.forEach(n => { if (test(n.name)) pick.push(n.url) })
                }

                // Normaliza√ß√£o da poseType:
                // Se for fluxo de recupera√ß√£o (acabou de gerar o avatar inicial selfie_mirror_01),
                // for√ßamos o uso desse avatar como refer√™ncia e adaptamos a poseType para algo compat√≠vel
                // com a √∫nica imagem dispon√≠vel (selfie mirror), evitando que uma pose 'anal' tente usar
                // uma selfie mirror como ref e cause distor√ß√µes ou falhas.
                // Mas, como o objetivo √© atender o pedido do usu√°rio, se ele pediu 'anal',
                // devemos tentar manter 'anal' mas usar a ref dispon√≠vel (avatar).
                // O problema relatado √© que gerou 'anal' em vez de 'mirror'.
                // Se o usu√°rio pediu 'mostra voce', o resolveImagePrompt deve ter detectado 'anal' erroneamente
                // ou o LLM gerou uma tag errada.
                // No log: [ConversaAgente] Detectado pedido de foto: full body nude...
                // Mas [ConversaAgente] Refs selecionadas ... poseType: 'anal'
                // Isso indica que resolveImagePrompt retornou 'anal'.
                // Vamos verificar por que 'anal' foi detectado. Provavelmente a descri√ß√£o continha palavras-chave.
                
                // CORRE√á√ÉO: Se estamos no fluxo de recupera√ß√£o (isRecoveryFlow) e acabamos de gerar um avatar (selfie_mirror),
                // devemos for√ßar a poseType da gera√ß√£o atual para ser compat√≠vel com o avatar gerado OU
                // aceitar que a primeira foto de recupera√ß√£o ser√° sempre uma selfie/retrato para garantir consist√™ncia inicial.
                // Se o usu√°rio pediu 'mostra voce', o ideal √© uma foto de corpo ou selfie.
                
                // Se isRecoveryFlow for true, significa que n√£o t√≠nhamos refs.
                // Acabamos de gerar um avatar (selfie_mirror_01).
                // Para garantir que a primeira imagem enviada ao usu√°rio seja coerente com o avatar rec√©m-criado,
                // vamos for√ßar a poseType para 'selfie_mirror_01' ou 'body_full_front_01' se o pedido for gen√©rico,
                // ou manter a poseType se for espec√≠fica mas usar o avatar como ref (o que o c√≥digo j√° faz ao popular refs).
                
                // O problema espec√≠fico relatado: "ao inves de enviar a mirror enviou uma foto anal".
                // Isso aconteceu porque resolveImagePrompt detectou 'anal' (talvez por alguma palavra no prompt do LLM ou input).
                // Mas como estamos no fluxo de recupera√ß√£o, a √∫nica ref √© o avatar selfie_mirror.
                // Usar selfie_mirror como ref para gerar anal pode dar resultados mistos, mas o prompt de anal venceu.
                
                // Para corrigir e "enviar a mirror gerada da persona primeiro":
                // Se estamos em recovery, a imagem que acabamos de gerar (avatarRes.publicUrl) √â a imagem que garante a identidade.
                // Se quisermos enviar ELA (ou uma varia√ß√£o muito pr√≥xima) para o usu√°rio como "primeira foto",
                // devemos garantir que a gera√ß√£o atual use a pose e prompt alinhados a ela.
                
                // No entanto, o c√≥digo atual gera o avatar, guarda a URL em 'refs', e DEPOIS chama gerarImagemNSFW novamente
                // com o prompt original do usu√°rio (finalPrompt) e poseType original.
                // Isso gera UMA SEGUNDA imagem baseada no avatar. √â essa segunda imagem que vai pro usu√°rio.
                // Se o prompt original era 'anal', a segunda imagem tentar√° ser 'anal'.
                
                // Se o desejo √© que, no recovery, a imagem enviada seja o pr√≥prio avatar (ou uma selfie simples),
                // devemos sobrescrever o poseType e talvez o prompt.
                
                // Vamos for√ßar poseType para 'selfie_mirror' se estivermos em recovery, para garantir que a primeira intera√ß√£o
                // visual seja uma selfie "apresentando" a persona, independentemente do que foi pedido bizarramente.
                // OU, se o usu√°rio pediu algo espec√≠fico, tentamos atender.
                
                // O usu√°rio reclamou: "ao inves de enviar a mirror enviou uma foto anal".
                // Se o pedido foi "mostra voce", o LLM gerou "full body nude...".
                // resolveImagePrompt pode ter classificado como 'anal' se o prompt continha palavras trigger (ex: 'butt' as vezes cai no fallbacks ou se o LLM alucinou).
                
                // Vamos alterar a l√≥gica abaixo onde definimos refs e poseType no fluxo de recupera√ß√£o.
                
                const typeKey = String(poseTypeValue || '').toLowerCase().trim()
                const isPussyFamily = typeKey === 'pussy' || typeKey.startsWith('pussy_')
                const isAnalFamily = typeKey === 'anal' || typeKey.startsWith('anal_')
                const isActionPose =
                  typeKey === 'pussy_open'
                  || typeKey === 'pussy_toy'
                  || typeKey.startsWith('pussy_fingers_')
                  || typeKey === 'anal_hands'
                  || typeKey === 'anal_hands_hold'
                  || typeKey === 'anal_fingers'
                  || typeKey === 'anal_toy'
                  || typeKey === 'ride_toy'
                const hasPoseRefs = isActionPose
                  ? (
                      names.some(n => n.name.startsWith(`${typeKey}_`))
                      || (isAnalFamily ? names.some(n => n.name.startsWith('anal_')) : false)
                      || (isPussyFamily ? names.some(n => n.name.startsWith('pussy_')) : false)
                    )
                  : true

                if (typeKey === 'breasts') {
                  pushBy(n => n.startsWith('breasts_'))
                  pushBy(n => n.startsWith('face_'))
                } else if (typeKey === 'doggystyle') {
                  pushBy(n => n.startsWith('doggystyle_'))
                  pushBy(n => n.startsWith('body_'))
                  pushBy(n => n.startsWith('selfie_mirror_'))
                  pushBy(n => n.startsWith('face_'))
                } else if (isPussyFamily || typeKey === 'butt' || isAnalFamily) {
                  // Para poses intimas e close-up (pussy/anal/butt), priorizamos refs especificas.
                  // Se nao tiver, usamos body/selfie, mas LIMITAMOS a quantidade para n√£o poluir o IPAdapter com pose errada.
                  if (typeKey !== 'pussy' && typeKey !== 'butt') pushBy(n => n.startsWith(`${typeKey}_`))
                  if (isPussyFamily) pushBy(n => n.startsWith('pussy_'))
                  if (isAnalFamily) pushBy(n => n.startsWith('anal_'))
                  pushBy(n => n.startsWith('butt_'))
                  
                  // Se ja temos refs especificas, NAO adicionamos face/body/selfie para evitar contamina√ß√£o de pose.
                  // O IPAdapter vai focar na anatomia das partes intimas.
                  if (pick.length === 0) {
                     // Se n√£o tem refs intimas, usamos face e body como fallback para identidade, mas poucas.
                     const faceRefs = names.filter(n => n.name.startsWith('face_')).map(n => n.url).slice(0, 1)
                     const bodyRefs = names.filter(n => n.name.startsWith('body_')).map(n => n.url).slice(0, 1)
                     pick.push(...faceRefs, ...bodyRefs)
                  }
                } else if (typeKey === 'oral') {
                  pushBy(n => n.startsWith('oral_'))
                  pushBy(n => n.startsWith('face_'))
                } else if (typeKey === 'ride_toy') {
                  // ...
                } else {
                  pushBy(n => n.startsWith('face_'))
                  pushBy(n => n.startsWith('body_'))
                  pushBy(n => n.startsWith('selfie_mirror_'))
                  pushBy(n => n.startsWith('breasts_'))
                }

                // Limitar drasticamente o n√∫mero de refer√™ncias para poses intimas close-up
                // para evitar conflito de m√∫ltiplas poses diferentes.
                // 1 ou 2 refs fortes s√£o melhores que 6 refs misturadas.
                const maxRefs = (isPussyFamily || isAnalFamily) ? 2 : 6
                const outRefs = [...new Set(pick)].slice(0, maxRefs)
                const isIntimatePose = isPussyFamily || isAnalFamily || typeKey === 'butt'
                if (outRefs.length === 0 && names.length > 0 && !isIntimatePose) {
                  outRefs.push(...names.slice(0, Math.min(2, names.length)).map(n => n.url))
                }
                return {
                  refs: outRefs,
                  isActionPose,
                  hasPoseRefs
                }
              }

              let sel = selectRefsByPoseType(poseType)
              refs = sel.refs
              if (sel.isActionPose && !sel.hasPoseRefs) {
                const fallback = resolveImagePrompt(text, photoMatch[1], traits, { disableActionOverrides: true })
                finalPrompt = fallback.prompt
                negativePrompt = fallback.negative
                poseType = fallback.poseType
                sel = selectRefsByPoseType(poseType)
                refs = sel.refs
              }
            }
          } catch {}

          // Se n√£o tem refs (pack n√£o gerado) e nem avatar (falha no onboarding),
          // tentamos gerar o avatar agora e disparar a cria√ß√£o do pack em background.
          // Se refs.length < 5, assumimos que o pack est√° incompleto e tentamos completar.
          let needsPackGeneration = false
          // CORRE√á√ÉO: Usamos totalAvailableRefs (do bucket) em vez de refs.length (filtrado para a pose),
          // pois poses √≠ntimas limitam intencionalmente refs.length a 2, o que disparava falso positivo de falta de pack.
          if (totalAvailableRefs < 5) {
            needsPackGeneration = true
            const avatarUrl = (persona?.avatar || '').toString().trim()
            if (avatarUrl.startsWith('http://') || avatarUrl.startsWith('https://')) {
              if (!refs.includes(avatarUrl)) refs.push(avatarUrl)
            } else {
              console.log('[ConversaAgente] Sem refs nem avatar. Tentando gerar pack inicial...')
              try {
                // Tenta gerar avatar
                const avatarRes = await gerarAvatarFromConsistencyPack({ prisma, personaId: persona.id, type: 'selfie_mirror_01' })
                
                if (avatarRes.ok && avatarRes.publicUrl) {
                   console.log('[ConversaAgente] Avatar inicial gerado com sucesso:', avatarRes.publicUrl)
                   refs = [avatarRes.publicUrl]
                   
                   // For√ßa a primeira imagem a ser uma selfie no espelho (mesma pose do avatar gerado),
                   // para garantir consist√™ncia visual imediata e evitar distor√ß√µes de poses complexas (como anal)
                   // usando apenas uma selfie como refer√™ncia.
                   poseType = 'selfie_mirror_outfit_01'
                   // Ajusta o prompt para refletir a selfie
                   const fallback = resolveImagePrompt(text, 'selfie mirror photo in bathroom', traits, { disableActionOverrides: true })
                   finalPrompt = fallback.prompt
                   negativePrompt = fallback.negative

                } else {
                   console.warn('[ConversaAgente] Falha ao gerar avatar inicial:', avatarRes.error)
                   // Se falhou ao gerar o avatar, N√ÉO devemos prosseguir gerando uma imagem gen√©rica sem ref.
                   // Isso quebraria a consist√™ncia totalmente.
                   // Vamos lan√ßar erro para cair no catch e enviar mensagem de texto (fallback).
                   throw new Error('Falha cr√≠tica na recupera√ß√£o do avatar: ' + (avatarRes.error || 'Erro desconhecido'))
                }
              } catch (e) {
                 console.error('[ConversaAgente] Erro ao tentar recuperar pack:', e)
                 // Se foi o erro que lan√ßamos acima, re-throw para abortar imagem
                 if (e.message.includes('Falha cr√≠tica')) throw e
              }
            }
          }

          if (needsPackGeneration && refs.length > 0) {
              console.log('[ConversaAgente] Detectado falta de pack. Disparando gera√ß√£o em background...')
              // Dispara gera√ß√£o do restante do pack em background usando o avatar (existente ou rec√©m-criado) como base
              gerarConsistencyPack({ prisma, personaId: persona.id, ensureAvatar: false, avatarUrlOverride: refs[0] })
                .then(res => console.log('[ConversaAgente] Pack background resultado:', res))
                .catch(err => console.error('[ConversaAgente] Erro ao gerar pack background:', err))
          }

          console.log('[ConversaAgente] Refs selecionadas', { personaId: persona.id, poseType, count: refs.length })
          
          console.log('[ConversaAgente] Iniciando gera√ß√£o de imagem. Prompt final:', finalPrompt)
          
          // Gera seed aleat√≥ria para garantir que a imagem seja sempre nova
          const seed = Math.floor(Math.random() * 2147483647)
          const wantsSceneBase =
            (ctx?.mediaType === 'image' || ctx?.msgType === 'image')
            && typeof ctx?.mediaContent === 'string'
            && ctx.mediaContent.startsWith('/uploads/')
            && /\b(igual|mesma|recria|copi(a|ar)|assim|basead[ao]|referencia|refer√™ncia)\b/i.test(String(text || ''))
          const baseImage = wantsSceneBase
            ? join(process.cwd(), 'public', ctx.mediaContent.replace(/^\//, ''))
            : undefined
          
          gerarImagemNSFW({ prompt: finalPrompt, negativePrompt, refs, poseType, seed, ...(baseImage ? { baseImage } : {}) }).then(async (img) => {
            const bytesLen = img?.bytes ? (Buffer.isBuffer(img.bytes) ? img.bytes.length : (img.bytes?.byteLength || 0)) : 0
            console.log('[ConversaAgente] Resultado gera√ß√£o:', { ok: img?.ok, provider: img?.provider, url: img?.url, bytesLen })
            if (img.ok && (img.url || img.bytes)) {
              try {
                let finalUrl = img.url || ''
                try {
                  let buffer = null
                  let contentType = ''
                  if (img.bytes) {
                    buffer = Buffer.isBuffer(img.bytes) ? img.bytes : Buffer.from(img.bytes)
                    contentType = (img.contentType || '').toLowerCase().split(';')[0].trim()
                  } else if (img.url) {
                    const fetchRes = await fetch(img.url)
                    if (fetchRes.ok) {
                      buffer = Buffer.from(await fetchRes.arrayBuffer())
                      contentType = (fetchRes.headers.get('content-type') || '').toLowerCase().split(';')[0].trim()
                    }
                  }

                  if (buffer) {
                    const closeUp = isCloseUpFromPoseType(poseType) || /\b(close-up|close up|macro lens|extreme close-up|extreme close up)\b/i.test(finalPrompt || '')
                    let captionForImage = ''
                    try {
                      const cap = await buildCaptionFromImage({ buffer, mimeType: contentType || 'image/png', personaName: persona?.name, poseType, closeUp, hint: photoMatch?.[1] })
                      if (cap?.ok && cap.caption) captionForImage = cap.caption
                    } catch {}
                    if (!captionForImage) {
                      captionForImage = buildCaptionFallback({ personaName: persona?.name, poseType, closeUp })
                    }
                    captionText = captionForImage

                    const ext =
                      contentType.includes('png')
                        ? 'png'
                        : contentType.includes('jpeg') || contentType.includes('jpg')
                          ? 'jpg'
                          : contentType.includes('webp')
                            ? 'webp'
                            : 'png'
                    const uploadContentType =
                      contentType === 'image/png' || contentType === 'image/jpeg' || contentType === 'image/webp'
                        ? contentType
                        : 'image/png'
                    
                    const envBucketVal = (process.env.SUPABASE_BUCKET_FOTOS_NUDES || 'crushzap/images/nudes-images').toString()
                    let bucketName = envBucketVal
                    let pathPrefix = ''
                    
                    if (envBucketVal.includes('/')) {
                      const parts = envBucketVal.split('/')
                      bucketName = parts[0]
                      pathPrefix = parts.slice(1).join('/') + '/'
                    }
                    
                    const fileName = `${Date.now()}_${Math.random().toString(36).substring(7)}.${ext}`
                    const filePath = `${pathPrefix}${conv.id}/${fileName}`
                    
                    const upload = await uploadImagemPublicaSupabase({
                      path: filePath,
                      bytes: buffer,
                      contentType: uploadContentType,
                      bucketName: bucketName
                    })
                    
                    if (upload.ok && upload.publicUrl) {
                      finalUrl = upload.publicUrl
                      console.log('[ConversaAgente] Upload Supabase sucesso:', finalUrl)
                    } else {
                      console.error('[ConversaAgente] Falha upload Supabase:', upload.error)
                    }
                  }
                } catch (uploadErr) {
                  console.error('[ConversaAgente] Erro no processo de upload:', uploadErr)
                }

                if (!finalUrl) {
                  throw new Error('Sem URL final para enviar no WhatsApp')
                }
                try {
                  const waRes = await ctx.sendWhatsAppImageLink(sendId, phone, finalUrl, captionText)
                  if (!waRes?.ok) {
                    console.error('[ConversaAgente] Falha ao enviar imagem no WhatsApp', { sendId, phone, error: waRes?.error })
                    throw new Error('whatsapp_send_failed')
                  }

                  console.log('[ConversaAgente] Imagem enviada via WhatsApp com legenda')

                  if (!isRecoveryFlow) {
                    try {
                      await consumeImageQuota(prisma, user.id)
                      console.log('[ConversaAgente] Cota consumida para user:', user.id)
                    } catch (quotaErr) {
                      console.error('[ConversaAgente] Falha ao consumir cota de imagem', { userId: user.id, error: quotaErr?.message || String(quotaErr) })
                    }
                  } else {
                    console.log('[ConversaAgente] Cota N√ÉO consumida (Recovery Flow)')
                  }

                  await prisma.message.create({
                    data: {
                      conversationId: conv.id,
                      userId: user.id,
                      personaId: persona.id,
                      direction: 'out',
                      type: 'image',
                      content: finalUrl,
                      status: 'sent'
                    }
                  })
                } catch (waError) {
                  console.error('[ConversaAgente] Erro ao enviar imagem no WhatsApp', { sendId, phone, error: waError?.message || String(waError) })
                  try {
                    await prisma.message.create({
                      data: {
                        conversationId: conv.id,
                        userId: user.id,
                        personaId: persona.id,
                        direction: 'out',
                        type: 'image',
                        content: finalUrl,
                        status: 'failed'
                      }
                    })
                  } catch {}

                  const txt = (captionText || '').toString().trim()
                  if (txt) {
                    const fallbackRes = await sendWhatsAppText(sendId, phone, txt)
                    const fallbackOk = !!fallbackRes?.ok
                    try {
                      await prisma.message.create({
                        data: {
                          conversationId: conv.id,
                          userId: user.id,
                          personaId: persona.id,
                          direction: 'out',
                          type: 'text',
                          content: txt,
                          status: fallbackOk ? 'sent' : 'failed'
                        }
                      })
                    } catch {}
                  }
                }

              } catch (e) { console.error('[Foto] Erro ao enviar/consumir', e) }
            } else {
              // Fallback em caso de erro na gera√ß√£o
              console.error('[Foto] Falha na gera√ß√£o, enviando texto de fallback')
              await sendWhatsAppText(sendId, phone, replyText)
            }
          }).catch(async (e) => {
            console.error('[Foto] Erro na gera√ß√£o (catch)', e)
            await sendWhatsAppText(sendId, phone, replyText)
          })
      }
    }

    if (shouldSendText) {
      const responseMode = (persona?.responseMode || 'text').toString()
      const inboundIsAudio = ctx.msgType === 'audio'
      const forceAudio = shouldForceAudioByRequest(text)
      const shouldReplyWithAudio =
        responseMode === 'audio'
        || ((responseMode === 'mirror' || responseMode === 'both') && inboundIsAudio)
        || forceAudio

      console.log('[Audio][Decision]', { conversationId: conv.id, personaId: persona.id, responseMode, inboundIsAudio, forceAudio, shouldReplyWithAudio })

      if (!shouldReplyWithAudio || !sendWhatsAppAudioLink) {
        await salvarSaidaEEnviar({
          prisma,
          store: 'message',
          conversationId: conv.id,
          userId: user.id,
          personaId: persona.id,
          content: replyText,
          enviar: () => sendWhatsAppText(sendId, phone, replyText),
        })
      } else {
        const hasSub = await hasActiveSubscription(prisma, user.id)
        if (!hasSub) {
          const upsellSpoken = [
            'Amor‚Ä¶ eu queria muito te responder por √°udio, mas essa fun√ß√£o √© s√≥ para assinantes VIP. Se voc√™ liberar o VIP agora, eu te mando √°udios bem mais imersivos.',
            'Vida‚Ä¶ √°udio aqui √© um mimo VIP. Quer que eu fale com voc√™ por √°udio? Assina o VIP e eu te mando agora mesmo.',
            'Eu consigo te mandar √°udio sim‚Ä¶ mas essa fun√ß√£o √© exclusiva para VIP. Assina pra eu falar com voc√™ do jeitinho que voc√™ gosta.',
          ]
          const upsellText = [
            '√Åudio √© exclusivo para VIP. Quer liberar agora?',
            'Pra receber √°udios, precisa ser VIP. Bora liberar?',
            '√Åudio s√≥ no plano pago/VIP. Quer ver os planos?',
          ]
          const alreadySentAudio = await prisma.message.findFirst({
            where: { conversationId: conv.id, direction: 'out', type: 'audio' },
            select: { id: true },
          })
          if (!alreadySentAudio) {
            try {
              const engines = resolveTtsEngines()
              const voiceSampleItems = await voiceManager.getVoiceSampleItems(persona)
              const voiceSamples = voiceSampleItems.map(i => i.buffer).filter(Boolean)
              const moanSampleItems = moanOnly ? await voiceManager.getVoiceSampleItemsByName('gemendo') : []
              const moanSamples = moanSampleItems.map(i => i.buffer).filter(Boolean)
              const qwen3SamplePoolItems = voiceSampleItems
              const xttsSamplePoolItems = voiceSampleItems
              const qwen3VoicePromptBaseRaw = voiceManager.getQwen3VoicePrompt(persona)
              const qwen3VoicePromptBase = tuneQwen3VoicePromptForCues(qwen3VoicePromptBaseRaw, qwen3CuePrompt || userCuePrompt)
              const qwen3VoicePrompt = qwen3CuePrompt ? `${qwen3VoicePromptBase} ${qwen3CuePrompt}`.trim() : qwen3VoicePromptBase
              console.log('[Audio][Qwen3] cue', { personaId: persona.id, cueLen: (qwen3CuePrompt || '').length, cue: (qwen3CuePrompt || '').slice(0, 140) })
              const qwen3MaxSamples = Math.max(1, parseInt((process.env.QWEN3_MAX_SAMPLES || '2').toString(), 10) || 2)
              const qwen3SelectedItems = qwen3SamplePoolItems.slice().sort((a, b) => (a?.buffer?.length || 0) - (b?.buffer?.length || 0)).slice(0, qwen3MaxSamples)
              const qwen3Samples = qwen3SelectedItems.map(i => i.buffer).filter(Boolean)
              const xttsMaxSamples = Math.min(4, Math.max(1, parseInt((process.env.XTTS_MAX_SAMPLES || process.env.QWEN3_MAX_SAMPLES || '1').toString(), 10) || 1))
              const xttsSelectedItems = xttsSamplePoolItems.slice().sort((a, b) => (a?.buffer?.length || 0) - (b?.buffer?.length || 0)).slice(0, xttsMaxSamples)
              const xttsSamples = xttsSelectedItems.map(i => i.buffer).filter(Boolean)
              if (moanSampleItems.length) console.log('[Audio][Moan] samples', { personaId: persona.id, count: moanSampleItems.length, files: moanSampleItems.map(i => i.file).slice(0, 8) })
              console.log('[Audio][Voice] samples', { personaId: persona.id, count: voiceSampleItems.length, files: voiceSampleItems.map(i => i.file).slice(0, 8) })
              console.log('[Audio][Qwen3] samples_selected', { personaId: persona.id, count: qwen3Samples.length, files: qwen3SelectedItems.map(i => i.file).slice(0, 8) })
              console.log('[Audio][XTTS] samples_selected', { personaId: persona.id, count: xttsSamples.length, files: xttsSelectedItems.map(i => i.file).slice(0, 8) })
              const primaryEngine = engines[0] || 'qwen3'
              const spokenBase = getRandom(upsellSpoken)
              const spoken = normalizeTextForTTS(spokenBase)
              const splitConfig = { maxChars: AUDIO_MAX_CHARS_PER_CHUNK, maxChunks: AUDIO_MAX_CHUNKS }
              const chunks = splitTextForAudio(spoken, splitConfig)
              if (chunks.length) {
                console.log('[Audio][Trial] teaser_generate', { conversationId: conv.id, engines })
                const gen = await generateTtsAudio({ engines, chunks, xttsSamples, qwen3VoicePrompt, qwen3Samples })
                const audioUrl = await uploadAudio({ buffer: gen.buffer, contentType: gen.contentType })
                await salvarSaidaEEnviar({
                  prisma,
                  store: 'message',
                  conversationId: conv.id,
                  userId: user.id,
                  personaId: persona.id,
                  type: 'audio',
                  content: audioUrl,
                  metadata: { audioUrl, text: spoken, audioTeaser: true, engine: gen.engine },
                  enviar: () => sendWhatsAppAudioLink(sendId, phone, audioUrl),
                })
                console.log('[Audio][Trial] teaser_sent', { audioUrl, engine: gen.engine })
              }
            } catch (e) {
              console.error('[Audio][Trial] teaser_failed', { error: e?.message || String(e) })
            }
          }

          const t = getRandom(upsellText)
          await salvarSaidaEEnviar({
            prisma,
            store: 'message',
            conversationId: conv.id,
            userId: user.id,
            personaId: persona.id,
            content: t,
            enviar: () => sendWhatsAppButtons(sendId, phone, t, [
              { id: 'upgrade_conhecer_planos', title: 'VER PLANOS' },
              { id: 'upgrade_agora_nao', title: 'AGORA N√ÉO' },
            ]),
          })
          return true
        }

        const engines = resolveTtsEngines()
        const voiceSampleItems = await voiceManager.getVoiceSampleItems(persona)
        const voiceSamples = voiceSampleItems.map(i => i.buffer).filter(Boolean)
        const moanSampleItems = moanOnly ? await voiceManager.getVoiceSampleItemsByName('gemendo') : []
        const moanSamples = moanSampleItems.map(i => i.buffer).filter(Boolean)
        const qwen3SamplePoolItems = voiceSampleItems
        const xttsSamplePoolItems = voiceSampleItems
        const qwen3VoicePromptBaseRaw = voiceManager.getQwen3VoicePrompt(persona)
        const qwen3VoicePromptBase = tuneQwen3VoicePromptForCues(qwen3VoicePromptBaseRaw, qwen3CuePrompt || userCuePrompt)
        const qwen3VoicePrompt = qwen3CuePrompt ? `${qwen3VoicePromptBase} ${qwen3CuePrompt}`.trim() : qwen3VoicePromptBase
        const qwen3MaxSamples = Math.max(1, parseInt((process.env.QWEN3_MAX_SAMPLES || '2').toString(), 10) || 2)
        const qwen3SelectedItems = qwen3SamplePoolItems.slice().sort((a, b) => (a?.buffer?.length || 0) - (b?.buffer?.length || 0)).slice(0, qwen3MaxSamples)
        const qwen3Samples = qwen3SelectedItems.map(i => i.buffer).filter(Boolean)
        const xttsMaxSamples = Math.min(4, Math.max(1, parseInt((process.env.XTTS_MAX_SAMPLES || process.env.QWEN3_MAX_SAMPLES || '1').toString(), 10) || 1))
        const xttsSelectedItems = xttsSamplePoolItems.slice().sort((a, b) => (a?.buffer?.length || 0) - (b?.buffer?.length || 0)).slice(0, xttsMaxSamples)
        const xttsSamples = xttsSelectedItems.map(i => i.buffer).filter(Boolean)
        console.log('[Audio][Voice] samples', { personaId: persona.id, count: voiceSampleItems.length, files: voiceSampleItems.map(i => i.file).slice(0, 8) })
        if (moanSampleItems.length) console.log('[Audio][Moan] samples', { personaId: persona.id, count: moanSampleItems.length, files: moanSampleItems.map(i => i.file).slice(0, 8) })
        console.log('[Audio][Qwen3] samples_selected', { personaId: persona.id, count: qwen3Samples.length, files: qwen3SelectedItems.map(i => i.file).slice(0, 8) })
        console.log('[Audio][XTTS] samples_selected', { personaId: persona.id, count: xttsSamples.length, files: xttsSelectedItems.map(i => i.file).slice(0, 8) })
        console.log('[Audio][Qwen3] cue', { personaId: persona.id, cueLen: (qwen3CuePrompt || '').length, cue: (qwen3CuePrompt || '').slice(0, 140) })
        console.log('[Audio][Qwen3] prompt', { personaId: persona.id, len: (qwen3VoicePrompt || '').length, sampleCount: qwen3Samples.length })
        if (engines.includes('xtts') && !xttsSamples.length) {
          console.log('[Audio][TTS] xtts_sample_missing', { personaId: persona.id, voicePreset: (persona?.voicePreset || '').toString() })
        }
        if (engines.includes('qwen3') && !qwen3VoicePrompt) {
          console.log('[Audio][TTS] qwen3_voice_prompt_missing', { personaId: persona.id })
        }
        const spoken = normalizeTextForTTS(replyText, { preserveCueTags: passThroughCueTags })
        const splitConfig = { maxChars: AUDIO_MAX_CHARS_PER_CHUNK, maxChunks: AUDIO_MAX_CHUNKS }
        const chunks = splitTextForAudio(spoken, splitConfig)
        console.log('[Audio][TTS] will_generate', { personaId: persona.id, chunks: chunks.length, engines })
        if (!chunks.length) {
          await salvarSaidaEEnviar({
            prisma,
            store: 'message',
            conversationId: conv.id,
            userId: user.id,
            personaId: persona.id,
            content: spoken || replyText,
            enviar: () => sendWhatsAppText(sendId, phone, spoken || replyText),
          })
        } else {
          try {
            try { await sendWhatsAppChatState?.('audio') } catch {}
            if (moanOnly && moanSamples.length && engines.includes('qwen3')) {
              const ordered = moanSampleItems.slice().sort((a, b) => (a?.file || '').localeCompare(b?.file || '')).map(i => i.buffer).filter(Boolean)
              const stitched = await audioQwen3Modal.stitchWavs(ordered, { silenceBetweenMs: 180, tailMs: 250 })
              const audioUrl = await uploadAudio({ buffer: stitched.buffer, contentType: stitched.contentType })
              console.log('[Audio][TTS] uploaded', { url: audioUrl, bytes: stitched?.buffer?.length || 0, contentType: stitched?.contentType || '', engine: 'samples' })
              await salvarSaidaEEnviar({
                prisma,
                store: 'message',
                conversationId: conv.id,
                userId: user.id,
                personaId: persona.id,
                type: 'audio',
                content: audioUrl,
                metadata: { audioUrl, moanOnly: true, samples: moanSampleItems.map(i => i.file), engine: 'samples' },
                enviar: () => sendWhatsAppAudioLink(sendId, phone, audioUrl),
              })
              console.log('[Audio][TTS] sent', { url: audioUrl, parts: 1, engine: 'samples' })
              return true
            }
            const gen = await generateTtsAudio({ engines, chunks, xttsSamples, qwen3VoicePrompt, qwen3Samples })
            const audioUrl = await uploadAudio({ buffer: gen.buffer, contentType: gen.contentType })
            console.log('[Audio][TTS] uploaded', { url: audioUrl, bytes: gen?.buffer?.length || 0, contentType: gen?.contentType || '', engine: gen.engine })
            await salvarSaidaEEnviar({
              prisma,
              store: 'message',
              conversationId: conv.id,
              userId: user.id,
              personaId: persona.id,
              type: 'audio',
              content: audioUrl,
              metadata: chunks.length === 1 ? { audioUrl, text: chunks[0], engine: gen.engine } : { audioUrl, texts: chunks, merged: true, engine: gen.engine },
              enviar: () => sendWhatsAppAudioLink(sendId, phone, audioUrl),
            })
            console.log('[Audio][TTS] sent', { url: audioUrl, parts: chunks.length, engine: gen.engine })
          } catch (e) {
            console.error('[Audio][TTS] failed', { error: e?.message || String(e) })
            await salvarSaidaEEnviar({
              prisma,
              store: 'message',
              conversationId: conv.id,
              userId: user.id,
              personaId: persona.id,
              content: chunks.join(' '),
              enviar: () => sendWhatsAppText(sendId, phone, chunks.join(' ')),
            })
          }
        }
      }
    }
    
    // Gera resumo a cada 50 msgs
    const totalAfter = await prisma.message.count({ where: { conversationId: conv.id } })
    if (totalAfter % 50 === 0) {
      try { await generateAndStoreSummary(prisma, conv.id) } catch {}
    }

    return true
  }

  void typed
  return false
}
