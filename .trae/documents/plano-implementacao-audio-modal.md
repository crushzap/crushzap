# Plano de Implementa√ß√£o: Aura Voice Engine (√Åudio Bidirecional com Modal)

Este documento descreve o plano de implementa√ß√£o para dotar o CrushZap de capacidades de audi√ß√£o (STT) e fala (TTS) com qualidade humana, utilizando **Modal** para processamento pesado de IA e **Node.js** para orquestra√ß√£o.

Objetivo: **Permitir que as personas ou√ßam √°udios do usu√°rio e respondam com voz natural, entona√ß√£o e personalidade, mantendo a lat√™ncia baixa.**

## Legenda de status das etapas
- üî¥ = n√£o iniciado
- üü° = em andamento
- ‚úÖ = conclu√≠do

Regra de atualiza√ß√£o: **cada etapa come√ßa com üî¥**. Quando iniciar uma etapa, mudo para üü°. Quando finalizar e validar, mudo para ‚úÖ.

---

## Regras R√≠gidas (N√£o Negoci√°veis)
1.  **Compatibilidade:** O sistema deve continuar funcionando para usu√°rios que s√≥ usam texto. O √°udio √© um "power-up".
2.  **Performance:** O tempo de "sil√™ncio" entre o usu√°rio enviar um √°udio e receber a resposta deve ser mascarado com feedback visual (status "gravando √°udio..." no WhatsApp).
3.  **Custo/Limites:** A gera√ß√£o de √°udio consome GPU. Deve ser contabilizada com peso maior que texto no sistema de cotas (Sugest√£o: 1 √Åudio = 10 Cr√©ditos de Texto).
4.  **Estabilidade:** Falhas no Modal (timeout/erro) devem degradar graciosamente para resposta em texto. Nunca deixar o usu√°rio sem resposta.
5.  **Storage:** Nenhum arquivo de √°udio deve ser armazenado permanentemente no servidor local; usar sempre Supabase Storage.
6.  **Windows/PowerShell:** Scripts e comandos devem ser compat√≠veis com PowerShell (sem `&&`).
7.  **Banco de dados:** Nunca resetar o banco. Nunca executar comandos `npx prisma` a partir do agente.

---

## Decis√µes de Arquitetura e Produto

### 1. Infraestrutura de IA (Modal)
-   **STT (Ouvir):** `OpenAI Whisper v3 Large`. Roda r√°pido em GPU e entende portugu√™s/g√≠rias muito bem.
-   **TTS (Falar):** `Coqui XTTS-v2`.
    -   Suporta **emo√ß√£o** e **entoa√ß√£o** baseada no contexto.
    -   Nativo em **PT-BR**.
    -   Permite **Voice Cloning** instant√¢neo (necess√°rio para os pre-sets).

### 2. Estrat√©gia de Vozes (Pre-sets)
Em vez de deixar o usu√°rio fazer upload de vozes (arriscado e complexo), teremos um **Cat√°logo de Vozes** curado:
-   Mapeamento fixo: `Personalidade` ‚Üí `Voz Base`.
-   Exemplo:
    -   *Sedutora* ‚Üí Voz aveludada, mais lenta.
    -   *Brincalhona* ‚Üí Voz mais aguda, r√°pida e energ√©tica.
    -   *Dominante* ‚Üí Voz firme, grave.
-   *T√©cnica:* Armazenamos arquivos `.wav` de refer√™ncia (6s) no projeto (`server/assets/voices/`) e enviamos para o Modal na hora da gera√ß√£o.

### 3. Modos de Resposta
Campo `responseMode` na tabela `Persona`:
-   `TEXT_ONLY`: Nunca manda √°udio.
-   `MIRROR` (Padr√£o):
    -   Usu√°rio manda Texto ‚Üí Responde Texto.
    -   Usu√°rio manda √Åudio ‚Üí Responde √Åudio.
    -   Usu√°rio pede √Åudio ("me manda um √°udio") ‚Üí Responde √Åudio (override via prompt).
-   `ALWAYS_AUDIO`: Responde tudo em √°udio (exceto listas/bot√µes).

Observa√ß√£o importante:
- A implementa√ß√£o atual no banco usa `ResponseMode` com valores `text`, `audio`, `both` e `mirror` (ver `prisma/schema.prisma`). O plano a seguir assume que `mirror` ser√° o comportamento padr√£o para √°udio.

### 4. Fluxo de Dados
1.  **WhatsApp (√Åudio)** ‚Üí Webhook Node.js.
2.  **Node.js** baixa m√≠dia ‚Üí Envia para **Modal (STT)**.
3.  **Modal** devolve Transcri√ß√£o.
4.  **Node.js** injeta transcri√ß√£o no prompt do Grok: `[USER_AUDIO]: "..."`.
5.  **Grok** gera resposta textual + tag de emo√ß√£o (opcional).
6.  **Node.js** verifica `responseMode`. Se for gerar √°udio:
    -   Envia Chat Action `recording_audio` para o WhatsApp.
    -   Envia texto + amostra de voz para **Modal (TTS)**.
    -   **Modal** devolve √°udio (buffer).
    -   **Node.js** sobe para **Supabase Storage**.
    -   **Node.js** envia URL p√∫blica para **WhatsApp**.

---

## Etapas (por ordem de execu√ß√£o)

### üü° Etapa 0 ‚Äî Prepara√ß√£o e Defini√ß√£o de Limites
**Objetivo:** Preparar terreno e definir custos.
-   Criar pasta `audios` no bucket do Supabase (A√ß√£o do Usu√°rio) e configurar o `.env`.
-   Definir constantes de custo no c√≥digo (ex: `COST_AUDIO_GENERATION = 10`).
-   Baixar/Gerar os arquivos de √°udio de refer√™ncia (samples) para as personalidades iniciais (Sedutora, Brincalhona, Dominante).

**Status atual**
- ‚úÖ `.env` j√° possui `SUPABASE_BUCKET_AUDIOS=crushzap/audios` (pasta j√° preparada pelo usu√°rio).
- üî¥ Ainda falta definir e aplicar regras de limite/custo para √°udio no controle de cota/trial.
- ‚úÖ `server/assets/voices/padrao.wav` adicionado (m√≠nimo vi√°vel para TTS funcionar).
- üî¥ Ainda falta adicionar presets adicionais (`sedutora.wav`, `dominante.wav`, `brincalhona.wav`) para variedade.
- ‚úÖ Trial: ao usu√°rio tentar √°udio (enviar √°udio ou pedir √°udio), bloquear TTS e enviar upsell (com teaser em √°udio na primeira vez).

### ‚úÖ Etapa 1 ‚Äî Infraestrutura Modal (Python)
**Objetivo:** Criar a API de IA que processa e gera √°udio.
-   Criar `infra/modal-audio/app.py`.
-   Implementar endpoint `transcribe` (Whisper).
-   Implementar endpoint `generate` (XTTS-v2).
-   Configurar *warm-up* para evitar "cold starts" muito longos (manter container aquecido ou aceitar delay inicial).

**Valida√ß√£o:**
-   Deploy no Modal (`modal deploy`).
-   Teste via curl/script: Enviar √°udio ‚Üí Receber texto. Enviar texto ‚Üí Receber √°udio.

**Status atual**
- ‚úÖ Deploy realizado com sucesso no Modal.
- ‚úÖ URLs do Modal (atuais):
  - Transcri√ß√£o (STT): `https://navibotlab--crushzap-audio-transcribe.modal.run`
  - Gera√ß√£o (TTS): `https://navibotlab--crushzap-audio-generate.modal.run`
- ‚úÖ Ajustes cr√≠ticos feitos para build est√°vel (Python 3.10, `coqui-tts==0.22.1`, patch no `coqpit` durante o build, aceite de licen√ßa CPML).
- ‚úÖ Script de deploy criado: `scripts/deploy-modal-audio.ps1`.

### ‚úÖ Etapa 2 ‚Äî Banco de Dados e Modelagem
**Objetivo:** Suportar configura√ß√£o de voz na Persona.
-   Adicionar campos no `schema.prisma` (Tabela `Persona`):
    -   `voicePreset` (String, default baseado na personalidade).
    -   `responseMode` (Enum: MIRROR, TEXT_ONLY, ALWAYS_AUDIO).
-   Executar migra√ß√£o (`npx prisma db push` ou generate, sem reset).

**Status atual**
- ‚úÖ Schema j√° foi atualizado para incluir `voicePreset` e `voiceSampleUrl` em `Persona`, e `mirror` em `ResponseMode`.
- ‚úÖ `npx prisma db push` executado (banco em sync com o schema).
- Observa√ß√£o: houve erro `EPERM unlink query_engine-windows.dll.node` durante o push (Windows lock). Se precisar regenerar client, rodar `npx prisma generate` com o server parado.

### ‚úÖ Etapa 3 ‚Äî Camada de Servi√ßo (Backend)
**Objetivo:** Abstrair a complexidade do Modal e Storage.
-   Criar `server/integracoes/ia/audio-modal.mjs`: Cliente para chamar a API do Modal.
-   Criar `server/integracoes/supabase/storage-audio.mjs`: Helpers para upload de √°udio tempor√°rio.
-   Criar `server/servicos/voice-manager.mjs`: L√≥gica para escolher o sample de voz baseado na persona.

**Status atual**
- ‚úÖ Cliente de √°udio criado e ajustado para usar `fetch` nativo (sem depend√™ncia de `axios`).
- ‚úÖ Storage de √°udio criado (Supabase).
- ‚úÖ `VoiceManager` criado para mapear preset/personalidade para `.wav` em `server/assets/voices/`.
- ‚úÖ Necess√°rio configurar no `.env`:
  - `MODAL_AUDIO_TRANSCRIBE_URL` (obrigat√≥rio)
  - `MODAL_AUDIO_GENERATE_URL` (obrigat√≥rio)

### ‚úÖ Etapa 4 ‚Äî Integra√ß√£o WhatsApp (Entrada/STT)
**Objetivo:** Permitir que o bot "ou√ßa".
-   Atualizar `server/rotas/whatsapp.rotas.mjs` para aceitar `type: audio`.
-   No handler de mensagem:
    -   Detectar √°udio.
    -   Baixar buffer.
    -   Chamar STT.
    -   Substituir o "corpo" da mensagem pela transcri√ß√£o, mas marcando flag `isAudio: true`.

**Status atual**
- ‚úÖ Implementado: √°udio recebido √© baixado, enviado para STT no Modal e a transcri√ß√£o √© injetada no `ctx.text` como `[O usu√°rio enviou um √°udio]. Transcri√ß√£o: "..."`.
- ‚úÖ Implementado: o √°udio recebido √© enviado ao Supabase (URL p√∫blica) e persistido como `type: audio`.

### ‚úÖ Etapa 5 ‚Äî Integra√ß√£o WhatsApp (Sa√≠da/TTS) e "C√©rebro"
**Objetivo:** Permitir que o bot "fale" e decida quando falar.
-   Atualizar `server/whatsapp/fluxos/conversa-agente.fluxo.mjs`:
    -   Logar transcri√ß√£o no hist√≥rico como `[√ÅUDIO]: texto`.
    -   L√≥gica de decis√£o `shouldReplyWithAudio`:
        -   Se `responseMode == ALWAYS_AUDIO`.
        -   Se `responseMode == MIRROR` E input foi √°udio.
        -   Se LLM solicitou (detectar tag `[SEND_AUDIO]` se implementarmos controle via prompt).
-   Se `shouldReplyWithAudio`:
    -   Disparar `sendChatState(recording_audio)`.
    -   Chamar TTS Service.
    -   Upload Supabase.
    -   Enviar mensagem de √°udio para usu√°rio.

**Itens que faltam nesta etapa**
- ‚úÖ Envio de √°udio via WhatsApp (upload-first + fallback por link).
- ‚úÖ Segmenta√ß√£o do texto em m√∫ltiplos √°udios (limite por chunk).
- ‚úÖ Contabiliza√ß√£o de custo por peso no c√°lculo de cota (assinatura).
- ‚úÖ On-demand: detectar quando usu√°rio pede √°udio e for√ßar TTS.
- üî¥ Pendente: enviar a√ß√£o de chat `recording_audio` antes do √°udio (otimiza√ß√£o UX).

### üî¥ Etapa 6 ‚Äî Dashboard e Ajustes Finais
**Objetivo:** Permitir configurar comportamento.
-   Atualizar p√°gina de Persona no frontend para exibir seletor de `Modo de Resposta`.
-   (Opcional neste momento) Seletor de voz manual (ou deixar autom√°tico pela personalidade por enquanto).

**Itens que faltam nesta etapa**
- Exibir no front presets dispon√≠veis (read-only) e modo de resposta.
- (Opcional) Campo para selecionar preset de voz manualmente.
- ‚úÖ Implementar player de √°udio no chat web (admin) para mensagens `type: audio`:
  - Play/Pause
  - Barra de progresso + seek
  - Velocidade (1x/1.5x/2x)
  - Onda/visualiza√ß√£o (waveform simplificado)
  - Baixar/link direto como fallback

---

## An√°lise de Custos Estimada (Refer√™ncia)

O Modal cobra por tempo de GPU.
-   **GPU T4 (Suficiente para infer√™ncia):** ~$0.59/hora.
-   **Whisper (STT):** Processa 1 min de √°udio em ~2-3 segundos. Custo √≠nfimo.
-   **XTTS (TTS):** Gera 10s de √°udio em ~3-5 segundos.
-   **Estimativa:**
    -   1 hora de GPU = 3600 segundos.
    -   Se cada √°udio gera 5s de processamento: ~700 √°udios por hora de GPU.
    -   Custo por √°udio: $0.59 / 700 ‚âà $0.0008 (menos de 1 centavo de d√≥lar).
    -   **Compara√ß√£o:** GPT-4 input/output √© mais caro que isso. Grok √© barato.
    -   **Conclus√£o:** O custo √© baixo, mas o risco √© o volume. O limite de 1:10 (1 √°udio = 10 textos) √© conservador e seguro para evitar abuso.

---

## Vari√°veis de Ambiente (Checklist)

Obrigat√≥rias (√°udio):
- `MODAL_AUDIO_TRANSCRIBE_URL=https://navibotlab--crushzap-audio-transcribe.modal.run`
- `MODAL_AUDIO_GENERATE_URL=https://navibotlab--crushzap-audio-generate.modal.run`
- `SUPABASE_BUCKET_AUDIOS=crushzap/audios`

Opcionais:
- `MODAL_AUDIO_URL` (apenas se quiser fornecer um prefixo √∫nico, mas no Modal atual os endpoints s√£o separados)

---

## Crit√©rios de Valida√ß√£o por Etapa (Obrigat√≥rios)

Sempre que marcar uma etapa como ‚úÖ:
- `node --check` nos arquivos `.mjs` tocados na etapa
- `npx tsc --noEmit` (para garantir que o front n√£o quebrou)
- `npm run build` (garante que o front continua compilando)
